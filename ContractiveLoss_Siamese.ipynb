{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "OUtikCEoQtu6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape images to include the channel dimension\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)"
      ],
      "metadata": {
        "id": "zmzl4otTQuX1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pairs(images, labels):\n",
        "    pairs = []\n",
        "    labels_pairs = []\n",
        "    num_classes = max(labels) + 1\n",
        "    digit_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "\n",
        "    for idx1 in range(len(images)):\n",
        "        x1, label1 = images[idx1], labels[idx1]\n",
        "\n",
        "        # Create a positive pair\n",
        "        idx2 = np.random.choice(digit_indices[label1])\n",
        "        x2 = images[idx2]\n",
        "        pairs.append([x1, x2])\n",
        "        labels_pairs.append(1)  # Label for positive pair\n",
        "\n",
        "        # Create a negative pair\n",
        "        label2 = np.random.randint(0, num_classes)\n",
        "        while label2 == label1:  # Ensure the label is different for negative pairs\n",
        "            label2 = np.random.randint(0, num_classes)\n",
        "        idx2 = np.random.choice(digit_indices[label2])\n",
        "        x2 = images[idx2]\n",
        "        pairs.append([x1, x2])\n",
        "        labels_pairs.append(0)  # Label for negative pair\n",
        "\n",
        "    return np.array(pairs), np.array(labels_pairs).astype(\"float32\")\n",
        "\n",
        "# Create pairs for training and testing\n",
        "pairs_train, labels_train = make_pairs(x_train, y_train)\n",
        "pairs_test, labels_test = make_pairs(x_test, y_test)"
      ],
      "metadata": {
        "id": "bN65SJxXQ41L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_network(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    return model\n",
        "\n",
        "# Register contrastive loss\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    squared_pred = K.square(y_pred)\n",
        "    squared_margin = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * squared_pred + (1 - y_true) * squared_margin)\n",
        "\n",
        "# Register the euclidean_distance function\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n"
      ],
      "metadata": {
        "id": "auS1ui5VRhV_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4BJNsTUbUdJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def siamese_network(input_shape):\n",
        "    base_network = create_base_network(input_shape)\n",
        "\n",
        "    input_a = layers.Input(shape=input_shape)\n",
        "    input_b = layers.Input(shape=input_shape)\n",
        "\n",
        "    output_a = base_network(input_a)\n",
        "    output_b = base_network(input_b)\n",
        "\n",
        "    # Calculate the Euclidean distance between embeddings\n",
        "    def euclidean_distance(vects):\n",
        "        x, y = vects\n",
        "        return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "    distance = layers.Lambda(euclidean_distance, output_shape=(1,))([output_a, output_b])\n",
        "\n",
        "    model = models.Model(inputs=[input_a, input_b], outputs=distance)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZY45r7eyUYur"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def siamese_network(input_shape):\n",
        "    base_network = create_base_network(input_shape)\n",
        "\n",
        "    input_a = layers.Input(shape=input_shape)\n",
        "    input_b = layers.Input(shape=input_shape)\n",
        "\n",
        "    output_a = base_network(input_a)\n",
        "    output_b = base_network(input_b)\n",
        "\n",
        "    distance = layers.Lambda(euclidean_distance, output_shape=(1,))([output_a, output_b])\n",
        "\n",
        "    model = models.Model(inputs=[input_a, input_b], outputs=distance)\n",
        "    return model\n",
        "\n",
        "input_shape = (28, 28, 1)  # Shape of MNIST images\n",
        "model = siamese_network(input_shape)\n",
        "model.compile(loss=contrastive_loss, optimizer='adam')"
      ],
      "metadata": {
        "id": "hBnSMX7yUhYD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, pairs, labels, epochs=20, batch_size=64):\n",
        "    images_a = np.array([pair[0] for pair in pairs]).reshape(-1, 28, 28, 1)\n",
        "    images_b = np.array([pair[1] for pair in pairs]).reshape(-1, 28, 28, 1)\n",
        "\n",
        "    model.fit([images_a, images_b], labels,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_split=0.2)  # Use a validation split\n",
        "\n",
        "# Train the model\n",
        "train_model(model, pairs_train, labels_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pFZjKQKUiBl",
        "outputId": "4d7a2cf8-d64f-4f96-e66b-e1c274e304a2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 99ms/step - loss: 0.0724 - val_loss: 0.0260\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 97ms/step - loss: 0.0235 - val_loss: 0.0208\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 96ms/step - loss: 0.0178 - val_loss: 0.0175\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 95ms/step - loss: 0.0142 - val_loss: 0.0151\n",
            "Epoch 5/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 96ms/step - loss: 0.0123 - val_loss: 0.0146\n",
            "Epoch 6/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 97ms/step - loss: 0.0110 - val_loss: 0.0130\n",
            "Epoch 7/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 96ms/step - loss: 0.0097 - val_loss: 0.0123\n",
            "Epoch 8/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 97ms/step - loss: 0.0089 - val_loss: 0.0121\n",
            "Epoch 9/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 97ms/step - loss: 0.0081 - val_loss: 0.0121\n",
            "Epoch 10/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 97ms/step - loss: 0.0076 - val_loss: 0.0114\n",
            "Epoch 11/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 95ms/step - loss: 0.0072 - val_loss: 0.0108\n",
            "Epoch 12/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 97ms/step - loss: 0.0067 - val_loss: 0.0109\n",
            "Epoch 13/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 97ms/step - loss: 0.0065 - val_loss: 0.0109\n",
            "Epoch 14/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 97ms/step - loss: 0.0061 - val_loss: 0.0106\n",
            "Epoch 15/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 96ms/step - loss: 0.0058 - val_loss: 0.0103\n",
            "Epoch 16/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 97ms/step - loss: 0.0056 - val_loss: 0.0098\n",
            "Epoch 17/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 96ms/step - loss: 0.0052 - val_loss: 0.0097\n",
            "Epoch 18/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 97ms/step - loss: 0.0052 - val_loss: 0.0094\n",
            "Epoch 19/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 95ms/step - loss: 0.0051 - val_loss: 0.0097\n",
            "Epoch 20/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 96ms/step - loss: 0.0048 - val_loss: 0.0094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Save the model\n",
        "model.save('siamese_network_model.h5')  # Save the model to a file\n",
        "print(\"Model saved as 'siamese_network_model.h5'\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model('siamese_network_model.h5',\n",
        "                          custom_objects={'contrastive_loss': contrastive_loss,\n",
        "                                          'euclidean_distance': euclidean_distance})\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, pairs, labels):\n",
        "    images_a = np.array([pair[0] for pair in pairs]).reshape(-1, 28, 28, 1)\n",
        "    images_b = np.array([pair[1] for pair in pairs]).reshape(-1, 28, 28, 1)\n",
        "\n",
        "    predictions = model.predict([images_a, images_b])\n",
        "    threshold = 0.5  # You might need to adjust this threshold based on your data\n",
        "    predicted_labels = (predictions < threshold).astype(int)\n",
        "\n",
        "    accuracy = np.mean(predicted_labels.flatten() == labels)\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Evaluate the model using the test pairs\n",
        "evaluate_model(loaded_model, pairs_test, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYlX73uTUkvh",
        "outputId": "ee0f3591-5e82-45f9-d5e3-4710f0f26267"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'siamese_network_model.h5'\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step\n",
            "Accuracy: 98.48%\n"
          ]
        }
      ]
    }
  ]
}